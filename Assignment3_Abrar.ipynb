{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 3: Intro to Recurrent Neural Networks: Math, Training, and the Copy Task\n",
        "\n",
        "# Instructor: Dr. Ankur Mali\n",
        "# University of South Florida (Spring 2025)\n",
        "# Student: Abrar Zahin\n",
        "\n",
        "#Objective\n",
        "The objective of this experiment is to compare the ability of the different model architectures of RNN  to perform a copy task.\n",
        "\n",
        "A copy task is a simple task that tests a model's ability to remember sequences of data over a time period. RNN's are able to complete this task due to their ability to maintain a memory through their hidden state and Back Propagation through time (BPTT).\n",
        "\n",
        "\n",
        "\n",
        "The models are:\n",
        "- Standard LSTM\n",
        "- Multiplicative LSTM\n",
        "- Standard GRU\n",
        "- Multiplicative GRU\n",
        "\n",
        "These gated architectures are used to address the issues faced by vanilla RNNs. While processing long sequences over time, the gradients of RNNs tend to converge towards zero (for vanishing) or towards infinity (for exploding). Thus, learning long range dependencies is inhibited.\n",
        "\n",
        "The LSTM models maintain a memory cell and the GRU uses a gated architecuture to allow long-term dependencies. The multiplicative variants of LSTM and GRU use\n",
        "\n",
        "#Experimental procedure\n",
        "We begin by implementing the four architectures: Standard LSTM, Multiplicative LSTM, Standard GRU and Multiplicative GRU.\n",
        "\n",
        "**CopyTaskDataset:**\n",
        "\n",
        "We begin by generating a random dataset for the copytask. In the class CopyTaskDataset(Dataset) we generated a sequence of random integers using NumPy. The sequence is of a certain length and vocabulary size (types of tokens) and the amount of these sequences (num_samples) is specified as well. In the input sequence, the sequence is generated followed by an equal number of delimiters. In the target label, the delimiters are returned and the original sequence follows.\n",
        "\n",
        "The input sequence is then added to a vector called data that is converted to a pytorch tensor of shape [num_samples, seq_length, 1]\n",
        "The target sequence is also added to a vector called labels and converted to a tensor of the same shape.\n",
        "\n",
        "Since we're using CrossEntropyLoss calculation for the training, we're also performing one hot encoding on the labels. CrossEntropyLoss expects one hot encoded labels.\n",
        "\n",
        "**LSTMCellPytorch:**\n",
        "\n",
        "In a function called LSTMPytorch, we define the model by initializing all the weight, bias, inputs and outputs. These parameters are first defined as zeros and transformed into their respective shapes for calculations.\n",
        "\n",
        "The forward function contains all the equations for a single forward pass of the model cell.\n",
        "\n",
        "**LSTMPytorch**\n",
        "\n",
        "The LSTMPytorch function unrolls the model over each full sequence. At first we intialize the hidden and cell state to retain the memory. Using a for loop, we go through all time steps one at a time. Thus, the output of each time step is used to derive the new hidden state.\n",
        "\n",
        "**train_lstm:**\n",
        "\n",
        "The train_lstm function trains the model using a optimizer, and training the model in epochs. In each epoch, loss is calculated. The optimizer we use is sigmoid function.\n",
        "\n",
        "**evaluate_lstm:**\n",
        "\n",
        "This function tests the model. The sample is divided into chunks and a hidded state is maintained sequentially through the chunks. The accuracy is calculated through accumulating the accuracies throughout the chunks.\n",
        "\n",
        "#Training and testing:\n",
        "\n",
        "We train and test the models with the same parameters thorugh different sequence lengths T = {100, 200, 500, 1000}.\n",
        "\n",
        "**Metrics**:\n",
        "We calculate the loss for each epoch during training and calculate the mean loss. During testing, we calculate the accuracy.\n",
        "\n",
        "For all four models, we compare these metrics for four different sequence lengths.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PuInr3Sk0J3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader  #Dataset and Dataloader used for training data\n",
        "import time                   #For training time calculation\n",
        "import numpy as np\n",
        "\n",
        "#This function generates a dataset of integer symbols, to be used for training the models\n",
        "class CopyTaskDataset(Dataset):\n",
        "    def __init__(self, seq_length=100, vocab_size=10, num_samples=1000):\n",
        "        self.seq_length = seq_length            #1000 pairs of sequences of len 100 and 10 type of tokens\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_samples = num_samples\n",
        "        self.delimiter_token = vocab_size\n",
        "        self.input_size = vocab_size + 1             #Number of unique tokens + Delimiter\n",
        "        self.data, self.labels = self.generate_data()\n",
        "\n",
        "    def generate_data(self):\n",
        "        data = []\n",
        "        labels = []\n",
        "        for _ in range(self.num_samples):\n",
        "            sequence = np.random.randint(0, self.vocab_size, size=(self.seq_length))\n",
        "\n",
        "            input_seq = np.concatenate([sequence, [self.delimiter_token] * self.seq_length])  #Original sequence first and delimiter tokens follow\n",
        "\n",
        "            target_seq = np.concatenate([[self.delimiter_token] * self.seq_length, sequence])   #Delimiter tokens first and original sequence follows\n",
        "\n",
        "            data.append(input_seq)\n",
        "            labels.append(target_seq)\n",
        "\n",
        "        # We reshape the data to have 3 dimensions and convert them to pytorch tensors (num_samples, seq_length, 1)\n",
        "        data = torch.tensor(data, dtype=torch.long)\n",
        "        labels = torch.tensor(labels, dtype=torch.long)  #long indices for CrossEntropy calculation\n",
        "\n",
        "        data_one_hot = torch.nn.functional.one_hot(data, num_classes=self.input_size).float()      #One hot encoding for training\n",
        "        #labels_one_hot = torch.nn.functional.one_hot(labels, num_classes=self.input_size).float()\n",
        "        return data_one_hot, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples       #This function returns the length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]     #To get the indices\n",
        "\n",
        "\n",
        "###\n",
        "###LSTM Implementation\n",
        "class LSTMCellPytorch(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A single-step RNN cell in PyTorch.\n",
        "    h_t = tanh( x_t * W_x + h_{t-1} * W_h + b )\n",
        "\n",
        "    LSTM:\n",
        "    i_t = sigmoid(x_t * W_xi + h_{t-1} * U_i + b_i)\n",
        "    f_t = sigmoid(x_t * W_xf + h_{t-1} * U_f + b_f)\n",
        "    o_t = sigmoid(x_t * W_xo + h_{t-1} * U_o + b_o)\n",
        "    c~_t = tanh(x_t * W_xc + h_{t-1} * U_c + b_c)\n",
        "    c_t = f_t * c_{t-1} + i_t * c~_t\n",
        "    h_t = o_t * tanh(c_t)\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        #Initialize the parameters used\n",
        "        #input and forget gates:\n",
        "        self.W_xi = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.U_i = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.W_xf = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.U_f = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_i = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.b_f = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        #output gate:\n",
        "        self.W_xo = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.U_o = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_o = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        #Memory cell gate:\n",
        "        self.W_xc = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.U_c = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_c = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "    def forward(self, x_t, h_prev, c_prev):\n",
        "        i_t = torch.sigmoid(x_t @ self.W_xi + h_prev @ self.U_i + self.b_i)\n",
        "        f_t = torch.sigmoid(x_t @ self.W_xf + h_prev @ self.U_f + self.b_f)\n",
        "        o_t = torch.sigmoid(x_t @ self.W_xo + h_prev @ self.U_o + self.b_o)\n",
        "        c_t = f_t * c_prev + i_t * torch.tanh(x_t @ self.W_xc + h_prev @ self.U_c + self.b_c)\n",
        "        h_t = o_t * torch.tanh(c_t)\n",
        "        return i_t, f_t, o_t, c_t, h_t\n",
        "\n",
        "###\n",
        "#LSTM Unrolling over time function\n",
        "class LSTMPyTorch(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Unrolls the LSTM over a full sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn_cell = LSTMCellPytorch(input_size, hidden_size)\n",
        "        self.W_out = torch.nn.Parameter(torch.randn(hidden_size, input_size) * 0.1)\n",
        "\n",
        "        self.b_out = torch.nn.Parameter(torch.zeros(input_size))\n",
        "\n",
        "    def forward(self, X, hidden_state= None):\n",
        "        batch_size, seq_length, _ = X.shape\n",
        "\n",
        "        if hidden_state is None:\n",
        "            h = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "            c = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "        else:\n",
        "            h, c = hidden_state\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]  # Shap [batch_size, input_size]\n",
        "            i_t, f_t, o_t, c_t, h_t = self.rnn_cell(x_t, h, c)\n",
        "            out_t = h @ self.W_out + self.b_out\n",
        "            outputs.append(out_t.unsqueeze(1))  # shape [batch_size,1,input_size]\n",
        "\n",
        "            h, c = h_t, c_t\n",
        "        # Concatenate across time:\n",
        "        return torch.cat(outputs, dim=1), (h, c)  # [batch_size, seq_length, input_size]\n",
        "\n",
        "###\n",
        "\n",
        "# The training function- train_LSTM\n",
        "def train_LSTM(model, train_loader, epochs=10, lr=0.01):\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    hidden_state = None        #hidden state maintained during training\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_batches = 0\n",
        "\n",
        "        for X_batch, Y_batch in train_loader:\n",
        "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output, hidden_state = model(X_batch, hidden_state)\n",
        "            hidden_state = (hidden_state[0].detach(), hidden_state[1].detach())   #Detaches from previous hidden state to prevent exploding gradient\n",
        "\n",
        "            loss = criterion(output.view(-1, input_size), Y_batch.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_batches += 1\n",
        "\n",
        "        epoch_loss = total_loss / total_batches\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "    return time.time() - start_time, loss\n",
        "\n",
        "#The testing function- evaluate_LSTM:\n",
        "def evaluate_LSTM(model, test_loader, chunk_size = 100):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, Y_batch in test_loader:\n",
        "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "            batch_size, seq_length, _ = X_batch.shape\n",
        "            hidden_state = None      #hiddnt state to be maintained throughout the batches\n",
        "            num_chunks = seq_length // chunk_size\n",
        "\n",
        "            chunk_correct = []\n",
        "            chunk_total = []\n",
        "            outputs = []\n",
        "            for i in range(num_chunks):\n",
        "                start = i * chunk_size\n",
        "                end = (i + 1) * chunk_size\n",
        "\n",
        "                X_chunk = X_batch[:, start:end, :]\n",
        "                Y_chunk = Y_batch[:, start:end]\n",
        "\n",
        "                output, hidden_state = model(X_chunk, hidden_state)\n",
        "                hidden_state = (hidden_state[0].detach(), hidden_state[1].detach())\n",
        "                outputs.append(output)\n",
        "\n",
        "\n",
        "            outputs = torch.cat(outputs, dim=1)  # Reconstruct full sequence\n",
        "            predicted_labels = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            correct_predictions += (predicted_labels == Y_batch).sum().item()\n",
        "            total_predictions += predicted_labels.numel()\n",
        "\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "###\n",
        "#For training dataset:\n",
        "seq_length = 100\n",
        "vocab_size = 10\n",
        "num_samples = 1500\n",
        "batch_size = 64\n",
        "\n",
        "# Creating dataset and Dataloader for training:\n",
        "dataset = CopyTaskDataset(seq_length, vocab_size, num_samples)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
        "\n",
        "# Define model\n",
        "input_size = vocab_size + 1  # Including delimiter token\n",
        "hidden_size = 128\n",
        "model = LSTMPyTorch(input_size, hidden_size)\n",
        "\n",
        "# Train model:\n",
        "training_time, loss = train_LSTM(model, train_loader, epochs=10, lr=0.005)\n",
        "print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "print(f\"Mean Loss: {loss:.4f}\")\n",
        "###\n",
        "\n",
        "###\n",
        "#For testing dataset:\n",
        "seq_length = 1000\n",
        "vocab_size = 10\n",
        "num_samples = 1500\n",
        "batch_size = 64\n",
        "\n",
        "#Dataset and loader for testing:\n",
        "test_dataset = CopyTaskDataset(seq_length, vocab_size, num_samples)\n",
        "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
        "\n",
        "#Test the model:\n",
        "test_accuracy = evaluate_LSTM(model, test_loader, chunk_size = 100)\n",
        "print(f\"LSTM Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c95d1915-e1f4-41d6-cae8-e50c32abbecc",
        "id": "c6-Afh6Ep50s"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Loss: 2.308744\n",
            "Epoch 2/10 | Loss: 2.057651\n",
            "Epoch 3/10 | Loss: 1.831775\n",
            "Epoch 4/10 | Loss: 1.643091\n",
            "Epoch 5/10 | Loss: 1.449918\n",
            "Epoch 6/10 | Loss: 1.382073\n",
            "Epoch 7/10 | Loss: 1.360832\n",
            "Epoch 8/10 | Loss: 1.342116\n",
            "Epoch 9/10 | Loss: 1.329215\n",
            "Epoch 10/10 | Loss: 1.319866\n",
            "Training completed in 69.95 seconds\n",
            "Mean Loss: 1.3162\n",
            "LSTM Test Accuracy: 0.5458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Results\n",
        "\n",
        "**T = 100**\n",
        "\n",
        "Epoch 1/10 | Loss: 2.307068                                                           \n",
        "Epoch 2/10 | Loss: 2.052329 <br>\n",
        "Epoch 3/10 | Loss: 1.805295 <br>\n",
        "Epoch 4/10 | Loss: 1.594715<br>\n",
        "Epoch 5/10 | Loss: 1.412653<br>\n",
        "Epoch 6/10 | Loss: 1.366210<br>\n",
        "Epoch 7/10 | Loss: 1.349931<br>\n",
        "Epoch 8/10 | Loss: 1.335301<br>\n",
        "Epoch 9/10 | Loss: 1.324294<br>\n",
        "Epoch 10/10 | Loss: 1.315721<br>\n",
        "Training completed in 65.02 seconds<br>\n",
        "Mean Loss: 1.3121<br>\n",
        "LSTM Test Accuracy: 0.5460<br>\n",
        "\n",
        "**T=200**\n",
        "\n",
        "Epoch 1/10 | Loss: 2.300380<br>\n",
        "Epoch 2/10 | Loss: 2.021753<br>\n",
        "Epoch 3/10 | Loss: 1.737184<br>\n",
        "Epoch 4/10 | Loss: 1.494123<br>\n",
        "Epoch 5/10 | Loss: 1.383652<br>\n",
        "Epoch 6/10 | Loss: 1.361969<br>\n",
        "Epoch 7/10 | Loss: 1.344759<br>\n",
        "Epoch 8/10 | Loss: 1.331481<br>\n",
        "Epoch 9/10 | Loss: 1.321371<br>\n",
        "Epoch 10/10 | Loss: 1.313421<br>\n",
        "Training completed in 72.52 seconds<br>\n",
        "Mean Loss: 1.3093<br>\n",
        "LSTM Test Accuracy: 0.5461<br>\n",
        "\n",
        "**T=500**\n",
        "\n",
        "Epoch 1/10 | Loss: 2.321768<br>\n",
        "Epoch 2/10 | Loss: 2.088610<br>\n",
        "Epoch 3/10 | Loss: 1.870382<br>\n",
        "Epoch 4/10 | Loss: 1.686539<br>\n",
        "Epoch 5/10 | Loss: 1.494767<br>\n",
        "Epoch 6/10 | Loss: 1.382270<br>\n",
        "Epoch 7/10 | Loss: 1.360358<br>\n",
        "Epoch 8/10 | Loss: 1.342912<br>\n",
        "Epoch 9/10 | Loss: 1.329381<br>\n",
        "Epoch 10/10 | Loss: 1.319294<br>\n",
        "Training completed in 74.14 seconds<br>\n",
        "Mean Loss: 1.3152<br>\n",
        "LSTM Test Accuracy: 0.5456<br>\n",
        "\n",
        "**T=1000**\n",
        "\n",
        "        \n",
        "Epoch 1/10 | Loss: 2.308744 <br>\n",
        "Epoch 2/10 | Loss: 2.057651 <br>\n",
        "Epoch 3/10 | Loss: 1.831775<br>\n",
        "Epoch 4/10 | Loss: 1.643091<br>\n",
        "Epoch 5/10 | Loss: 1.449918<br>\n",
        "Epoch 6/10 | Loss: 1.382073<br>\n",
        "Epoch 7/10 | Loss: 1.360832<br>\n",
        "Epoch 8/10 | Loss: 1.342116<br>\n",
        "Epoch 9/10 | Loss: 1.329215<br>\n",
        "Epoch 10/10 | Loss: 1.319866<br>\n",
        "Training completed in 69.95 seconds<br>\n",
        "Mean Loss: 1.3162<br>\n",
        "LSTM Test Accuracy: 0.5458<br>"
      ],
      "metadata": {
        "id": "657c6TM_CZFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "#Multiplicative LSTM:\n",
        "\n",
        "Now we train and test a multiplicative LSTM cell with identical hyperparameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "KhbF1u1A5XQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader  #Dataset and Dataloader used for training data\n",
        "import time                   #For training time calculation\n",
        "import numpy as np\n",
        "\n",
        "#This function generates a dataset of integer symbols, to be used for training the models\n",
        "class CopyTaskDataset(Dataset):\n",
        "    def __init__(self, seq_length=100, vocab_size=10, num_samples=1000):\n",
        "        self.seq_length = seq_length            #1000 pairs of sequences of len 100 and 10 type of tokens\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_samples = num_samples\n",
        "        self.delimiter_token = vocab_size\n",
        "        self.input_size = vocab_size + 1             #Number of unique tokens + Delimiter\n",
        "        self.data, self.labels = self.generate_data()\n",
        "\n",
        "    def generate_data(self):\n",
        "        data = []\n",
        "        labels = []\n",
        "        for _ in range(self.num_samples):\n",
        "            sequence = np.random.randint(0, self.vocab_size, size=(self.seq_length))\n",
        "\n",
        "            input_seq = np.concatenate([sequence, [self.delimiter_token] * self.seq_length])  #Original sequence first and delimiter tokens follow\n",
        "\n",
        "            target_seq = np.concatenate([[self.delimiter_token] * self.seq_length, sequence])   #Delimiter tokens first and original sequence follows\n",
        "\n",
        "            data.append(input_seq)\n",
        "            labels.append(target_seq)\n",
        "\n",
        "        # We reshape the data to have 3 dimensions and convert them to pytorch tensors (num_samples, seq_length, 1)\n",
        "        data = torch.tensor(data, dtype=torch.long)\n",
        "        labels = torch.tensor(labels, dtype=torch.long)  #long indices for CrossEntropy calculation\n",
        "\n",
        "        data_one_hot = torch.nn.functional.one_hot(data, num_classes=self.input_size).float()      #One hot encoding for training\n",
        "        #labels_one_hot = torch.nn.functional.one_hot(labels, num_classes=self.input_size).float()\n",
        "        return data_one_hot, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "###\n",
        "###LSTM Implementation\n",
        "class LSTMCellPytorch(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A single-step RNN cell in PyTorch.\n",
        "    h_t = tanh( x_t * W_x + h_{t-1} * W_h + b )\n",
        "\n",
        "    Multiplicative LSTM:\n",
        "    m_t = sigmoid(x_t * W_xm + h_{t-1} * U_m + b_m)\n",
        "    ~x_t = m_t * x_t\n",
        "    i_t = sigmoid(~x_t * W_xi + h_{t-1} * U_i + b_i)\n",
        "    f_t = sigmoid(~x_t * W_xf + h_{t-1} * U_f + b_f)\n",
        "    o_t = sigmoid(~x_t * W_xo + h_{t-1} * U_o + b_o)\n",
        "    ~c_t = tanh(x_t * W_xc + h_{t-1} * U_c + b_c)\n",
        "    c_t = f_t * c_{t-1} + i_t * ~c_t\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        #Initialize the variables used\n",
        "\n",
        "        #memory matrix variables:\n",
        "        self.W_xm = torch.nn.Parameter(torch.randn(input_size, input_size) * 0.1)\n",
        "        self.U_m = torch.nn.Parameter(torch.randn(hidden_size, input_size) * 0.1)\n",
        "        self.b_m = torch.nn.Parameter(torch.zeros(input_size))\n",
        "\n",
        "        #input and forget gates:\n",
        "        self.W_xi = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.U_i = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.W_xf = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.U_f = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_i = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.b_f = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        #output gate:\n",
        "        self.W_xo = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.U_o = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_o = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        #Memory cell gate:\n",
        "        self.W_xc = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.U_c = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_c = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, x_t, h_prev, c_prev):\n",
        "\n",
        "        m_t = torch.sigmoid(x_t @ self.W_xm + h_prev @ self.U_m + self.b_m)         #memory matrix\n",
        "        x_t = m_t * x_t\n",
        "\n",
        "\n",
        "        i_t = torch.sigmoid(x_t @ self.W_xi + h_prev @ self.U_i + self.b_i)\n",
        "        f_t = torch.sigmoid(x_t @ self.W_xf + h_prev @ self.U_f + self.b_f)\n",
        "        o_t = torch.sigmoid(x_t @ self.W_xo + h_prev @ self.U_o + self.b_o)\n",
        "        c_t = f_t * c_prev + i_t * torch.tanh(x_t @ self.W_xc + h_prev @ self.U_c + self.b_c)\n",
        "        h_t = o_t * torch.tanh(c_t)\n",
        "        return m_t, i_t, f_t, o_t, c_t, h_t\n",
        "\n",
        "###\n",
        "#LSTM Unrolling over time function\n",
        "class LSTMPyTorch(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Unrolls the LSTM over a full sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn_cell = LSTMCellPytorch(input_size, hidden_size)\n",
        "        # Output projection to match the original input dimension for copy task\n",
        "        self.W_out = torch.nn.Parameter(torch.randn(hidden_size, input_size) * 0.1)\n",
        "\n",
        "        self.b_out = torch.nn.Parameter(torch.zeros(input_size))\n",
        "\n",
        "    def forward(self, X, hidden_state= None):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size, seq_length, _ = X.shape\n",
        "\n",
        "        if hidden_state is None:\n",
        "            h = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "            c = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "        else:\n",
        "            h, c = hidden_state\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]  # [batch_size, input_size]\n",
        "            m_t, i_t, f_t, o_t, c_t, h_t = self.rnn_cell(x_t, h, c)\n",
        "            # Project hidden -> input_size\n",
        "            out_t = h @ self.W_out + self.b_out\n",
        "            outputs.append(out_t.unsqueeze(1))  # shape [batch_size,1,input_size]\n",
        "\n",
        "            h, c = h_t, c_t\n",
        "        # Concatenate across time\n",
        "        return torch.cat(outputs, dim=1), (h, c)  # [batch_size, seq_length, input_size]\n",
        "\n",
        "###\n",
        "\n",
        "# The training function- train_LSTM\n",
        "def train_LSTM(model, train_loader, epochs=10, lr=0.01):\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    hidden_state = None        #hidden state maintained during training\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_batches = 0\n",
        "\n",
        "        for X_batch, Y_batch in train_loader:\n",
        "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output, hidden_state = model(X_batch, hidden_state)\n",
        "            hidden_state = (hidden_state[0].detach(), hidden_state[1].detach())   #Detaches from previous hidden state to prevent exploding gradient\n",
        "\n",
        "            loss = criterion(output.view(-1, input_size), Y_batch.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_batches += 1\n",
        "\n",
        "        epoch_loss = total_loss / total_batches\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "    return time.time() - start_time, loss\n",
        "\n",
        "#The testing function- evaluate_LSTM:\n",
        "def evaluate_LSTM(model, test_loader, chunk_size = 100):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, Y_batch in test_loader:\n",
        "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "            batch_size, seq_length, _ = X_batch.shape\n",
        "            hidden_state = None      #hiddnt state to be maintained throughout the batches\n",
        "            num_chunks = seq_length // chunk_size\n",
        "\n",
        "            chunk_correct = []\n",
        "            chunk_total = []\n",
        "            outputs = []\n",
        "            for i in range(num_chunks):\n",
        "                start = i * chunk_size\n",
        "                end = (i + 1) * chunk_size\n",
        "\n",
        "                X_chunk = X_batch[:, start:end, :]\n",
        "                Y_chunk = Y_batch[:, start:end]\n",
        "\n",
        "                output, hidden_state = model(X_chunk, hidden_state)\n",
        "                hidden_state = (hidden_state[0].detach(), hidden_state[1].detach())\n",
        "                outputs.append(output)\n",
        "\n",
        "\n",
        "            outputs = torch.cat(outputs, dim=1)  # Reconstruct full sequence\n",
        "            predicted_labels = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            correct_predictions += (predicted_labels == Y_batch).sum().item()\n",
        "            total_predictions += predicted_labels.numel()\n",
        "\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "###\n",
        "#For training dataset:\n",
        "seq_length = 100\n",
        "vocab_size = 10\n",
        "num_samples = 1500\n",
        "batch_size = 64\n",
        "\n",
        "# Creating dataset and Dataloader for training:\n",
        "dataset = CopyTaskDataset(seq_length, vocab_size, num_samples)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
        "\n",
        "# Define model\n",
        "input_size = vocab_size + 1  # Including delimiter token\n",
        "hidden_size = 128\n",
        "model = LSTMPyTorch(input_size, hidden_size)\n",
        "\n",
        "# Train model:\n",
        "training_time, loss = train_LSTM(model, train_loader, epochs=10, lr=0.005)\n",
        "print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "print(f\"Mean Loss: {loss:.4f}\")\n",
        "###\n",
        "\n",
        "###\n",
        "#For testing dataset:\n",
        "seq_length = 1000\n",
        "vocab_size = 10\n",
        "num_samples = 1500\n",
        "batch_size = 64\n",
        "\n",
        "#Dataset and loader for testing:\n",
        "test_dataset = CopyTaskDataset(seq_length, vocab_size, num_samples)\n",
        "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
        "\n",
        "#Test the model:\n",
        "test_accuracy = evaluate_LSTM(model, test_loader, chunk_size = 100)\n",
        "print(f\"mLSTM Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1c0ddb6-ae23-4632-8bbb-9f90fde0ad30",
        "id": "Fe_6BvcQBBEl"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Loss: 2.322238\n",
            "Epoch 2/10 | Loss: 2.096273\n",
            "Epoch 3/10 | Loss: 1.906069\n",
            "Epoch 4/10 | Loss: 1.796025\n",
            "Epoch 5/10 | Loss: 1.719152\n",
            "Epoch 6/10 | Loss: 1.584395\n",
            "Epoch 7/10 | Loss: 1.446875\n",
            "Epoch 8/10 | Loss: 1.407637\n",
            "Epoch 9/10 | Loss: 1.374652\n",
            "Epoch 10/10 | Loss: 1.354479\n",
            "Training completed in 100.52 seconds\n",
            "Mean Loss: 1.3472\n",
            "mLSTM Test Accuracy: 0.5448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**T = 100**\n",
        "Epoch 1/10 | Loss: 2.325448<br>\n",
        "Epoch 2/10 | Loss: 2.100564<br>\n",
        "Epoch 3/10 | Loss: 1.911989<br>\n",
        "Epoch 4/10 | Loss: 1.819186<br>\n",
        "Epoch 5/10 | Loss: 1.775007<br>\n",
        "Epoch 6/10 | Loss: 1.705509<br>\n",
        "Epoch 7/10 | Loss: 1.558481<br>\n",
        "Epoch 8/10 | Loss: 1.449990<br>\n",
        "Epoch 9/10 | Loss: 1.407998<br>\n",
        "Epoch 10/10 | Loss: 1.375835<br>\n",
        "Training completed in 92.10 seconds<br>\n",
        "Mean Loss: 1.3647<br>\n",
        "mLSTM Test Accuracy: 0.5434<br>\n",
        "\n",
        "\n",
        "**T = 200**\n",
        "Epoch 1/10 | Loss: 2.326527<br>\n",
        "Epoch 2/10 | Loss: 2.106548<br>\n",
        "Epoch 3/10 | Loss: 1.930772<br>\n",
        "Epoch 4/10 | Loss: 1.835865<br>\n",
        "Epoch 5/10 | Loss: 1.784955<br>\n",
        "Epoch 6/10 | Loss: 1.728241<br>\n",
        "Epoch 7/10 | Loss: 1.631638<br>\n",
        "Epoch 8/10 | Loss: 1.482542<br>\n",
        "Epoch 9/10 | Loss: 1.410738<br>\n",
        "Epoch 10/10 | Loss: 1.383396<br>\n",
        "Training completed in 97.40 seconds<br>\n",
        "Mean Loss: 1.3712<br>\n",
        "mLSTM Test Accuracy: 0.5437<br>\n",
        "<br>\n",
        "**T = 500**\n",
        "Epoch 1/10 | Loss: 2.324010<br>\n",
        "Epoch 2/10 | Loss: 2.098319<br>\n",
        "Epoch 3/10 | Loss: 1.911312<br>\n",
        "Epoch 4/10 | Loss: 1.806705<br>\n",
        "Epoch 5/10 | Loss: 1.742324<br>\n",
        "Epoch 6/10 | Loss: 1.641926<br>\n",
        "Epoch 7/10 | Loss: 1.479221<br>\n",
        "Epoch 8/10 | Loss: 1.414648<br>\n",
        "Epoch 9/10 | Loss: 1.383242<br>\n",
        "Epoch 10/10 | Loss: 1.359688<br>\n",
        "Training completed in 87.35 seconds<br>\n",
        "Mean Loss: 1.3518<br>\n",
        "mLSTM Test Accuracy: 0.5446<br>\n",
        "\n",
        "\n",
        "**T = 1000**\n",
        "Epoch 1/10 | Loss: 2.322238<br>\n",
        "Epoch 2/10 | Loss: 2.096273<br>\n",
        "Epoch 3/10 | Loss: 1.906069<br>\n",
        "Epoch 4/10 | Loss: 1.796025<br>\n",
        "Epoch 5/10 | Loss: 1.719152<br>\n",
        "Epoch 6/10 | Loss: 1.584395<br>\n",
        "Epoch 7/10 | Loss: 1.446875<br>\n",
        "Epoch 8/10 | Loss: 1.407637<br>\n",
        "Epoch 9/10 | Loss: 1.374652<br>\n",
        "Epoch 10/10 | Loss: 1.354479<br>\n",
        "Training completed in 100.52 seconds<br>\n",
        "Mean Loss: 1.3472<br>\n",
        "mLSTM Test Accuracy: 0.5448<br>"
      ],
      "metadata": {
        "id": "xtLyaDyIFmiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# GRU:\n",
        "We perform the same training and testing procedure on a GRU model for comparison.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LF2n1gDhRVvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader  #Dataset and Dataloader used for training data\n",
        "import time                   #For training time calculation\n",
        "import numpy as np\n",
        "\n",
        "#This function generates a dataset of integer symbols, to be used for training the models\n",
        "class CopyTaskDataset(Dataset):\n",
        "    def __init__(self, seq_length=100, vocab_size=10, num_samples=1000):\n",
        "        self.seq_length = seq_length            #1000 pairs of sequences of len 100 and 10 type of tokens\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_samples = num_samples\n",
        "        self.delimiter_token = vocab_size\n",
        "        self.input_size = vocab_size + 1             #Number of unique tokens + Delimiter\n",
        "        self.data, self.labels = self.generate_data()\n",
        "\n",
        "    def generate_data(self):\n",
        "        data = []\n",
        "        labels = []\n",
        "        for _ in range(self.num_samples):\n",
        "            sequence = np.random.randint(0, self.vocab_size, size=(self.seq_length))\n",
        "\n",
        "            input_seq = np.concatenate([sequence, [self.delimiter_token] * self.seq_length])  #Original sequence first and delimiter tokens follow\n",
        "\n",
        "            target_seq = np.concatenate([[self.delimiter_token] * self.seq_length, sequence])   #Delimiter tokens first and original sequence follows\n",
        "\n",
        "            data.append(input_seq)\n",
        "            labels.append(target_seq)\n",
        "\n",
        "        # We reshape the data to have 3 dimensions and convert them to pytorch tensors (num_samples, seq_length, 1)\n",
        "        data = torch.tensor(data, dtype=torch.long)\n",
        "        labels = torch.tensor(labels, dtype=torch.long)  #long indices for CrossEntropy calculation\n",
        "\n",
        "        data_one_hot = torch.nn.functional.one_hot(data, num_classes=self.input_size).float()      #One hot encoding for training\n",
        "        #labels_one_hot = torch.nn.functional.one_hot(labels, num_classes=self.input_size).float()\n",
        "        return data_one_hot, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "###\n",
        "###GRU Implementation\n",
        "class LSTMCellPytorch(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A single-step RNN cell in PyTorch for reference:\n",
        "    h_t = tanh( x_t * W_x + h_{t-1} * W_h + b )\n",
        "\n",
        "    GRU:\n",
        "    z_t = sigmoid(x_t * W_xz + h_{t-1} * W_hz + b_z)\n",
        "    r_t = sigmoid(x_t * W_xr + h_{t-1} * W_hr + b_r)\n",
        "    ~h_t = tanh(z_t * r_t * h_{t-1} + x_t * W_xh + b_h)\n",
        "    h_t = (1 - z_t) * h_{t-1} + z_t * ~h_t\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Update gate:\n",
        "        self.W_xz = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.U_hz = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_z = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Reset gate:\n",
        "        self.W_xr = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.U_hr = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_r = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Candidate hidden state:\n",
        "        self.W_xh = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.U_hh = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_h = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, x_t, h_prev):\n",
        "        z_t = torch.sigmoid(x_t @ self.W_xz + h_prev @ self.U_hz + self.b_z)\n",
        "\n",
        "\n",
        "        r_t = torch.sigmoid(x_t @ self.W_xr + h_prev @ self.U_hr + self.b_r)\n",
        "\n",
        "\n",
        "        h_candidate = torch.tanh(x_t @ self.W_xh + (r_t * h_prev) @ self.U_hh + self.b_h)\n",
        "\n",
        "\n",
        "        h_t = (1 - z_t) * h_prev + z_t * h_candidate\n",
        "\n",
        "        return z_t, r_t, h_candidate, h_t\n",
        "\n",
        "###\n",
        "#GRU Unrolling over time function\n",
        "class LSTMPyTorch(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Unrolls the GRU over a full sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn_cell = LSTMCellPytorch(input_size, hidden_size)\n",
        "        self.W_out = torch.nn.Parameter(torch.randn(hidden_size, input_size) * 0.1)\n",
        "\n",
        "        self.b_out = torch.nn.Parameter(torch.zeros(input_size))\n",
        "\n",
        "    def forward(self, X, hidden_state= None):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size, seq_length, _ = X.shape\n",
        "\n",
        "        if hidden_state is None:\n",
        "            h = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "\n",
        "        else:\n",
        "            h = hidden_state\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]  # [batch_size, input_size]\n",
        "            z_t, r_t, h_candidate, h_t = self.rnn_cell(x_t, h)\n",
        "            out_t = h @ self.W_out + self.b_out\n",
        "            outputs.append(out_t.unsqueeze(1))  # shape [batch_size,1,input_size]\n",
        "\n",
        "            h = h_t\n",
        "        # Concatenate across time\n",
        "        return torch.cat(outputs, dim=1), h  # [batch_size, seq_length, input_size]\n",
        "\n",
        "###\n",
        "\n",
        "# The training function- train_LSTM\n",
        "def train_LSTM(model, train_loader, epochs=10, lr=0.01):\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    hidden_state = None        #hidden state maintained during training\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_batches = 0\n",
        "\n",
        "        for X_batch, Y_batch in train_loader:\n",
        "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output, hidden_state = model(X_batch, hidden_state)\n",
        "            hidden_state = (hidden_state.detach())   #Detaches from previous hidden state to prevent exploding gradient\n",
        "\n",
        "            loss = criterion(output.view(-1, input_size), Y_batch.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_batches += 1\n",
        "\n",
        "        epoch_loss = total_loss / total_batches\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "    return time.time() - start_time, loss\n",
        "\n",
        "#The testing function- evaluate_LSTM:\n",
        "def evaluate_LSTM(model, test_loader, chunk_size = 100):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, Y_batch in test_loader:\n",
        "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "            batch_size, seq_length, _ = X_batch.shape\n",
        "            hidden_state = None      #hiddnt state to be maintained throughout the batches\n",
        "            num_chunks = seq_length // chunk_size\n",
        "\n",
        "            chunk_correct = []\n",
        "            chunk_total = []\n",
        "            outputs = []\n",
        "            for i in range(num_chunks):\n",
        "                start = i * chunk_size\n",
        "                end = (i + 1) * chunk_size\n",
        "\n",
        "                X_chunk = X_batch[:, start:end, :]\n",
        "                Y_chunk = Y_batch[:, start:end]\n",
        "\n",
        "                output, hidden_state = model(X_chunk, hidden_state)\n",
        "                hidden_state = (hidden_state.detach())\n",
        "                outputs.append(output)\n",
        "\n",
        "\n",
        "            outputs = torch.cat(outputs, dim=1)  # Reconstruct full sequence\n",
        "            predicted_labels = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            correct_predictions += (predicted_labels == Y_batch).sum().item()\n",
        "            total_predictions += predicted_labels.numel()\n",
        "\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "###\n",
        "#For training dataset:\n",
        "seq_length = 100\n",
        "vocab_size = 10\n",
        "num_samples = 1500\n",
        "batch_size = 64\n",
        "\n",
        "# Creating dataset and Dataloader for training:\n",
        "dataset = CopyTaskDataset(seq_length, vocab_size, num_samples)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
        "\n",
        "# Define the model:\n",
        "input_size = vocab_size + 1  # Including delimiter token\n",
        "hidden_size = 128\n",
        "model = LSTMPyTorch(input_size, hidden_size)\n",
        "\n",
        "# Train model:\n",
        "training_time, loss = train_LSTM(model, train_loader, epochs=10, lr=0.005)\n",
        "print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "print(f\"Mean Loss: {loss:.4f}\")\n",
        "###\n",
        "\n",
        "###\n",
        "#For testing dataset:\n",
        "seq_length = 1000\n",
        "vocab_size = 10\n",
        "num_samples = 1500\n",
        "batch_size = 64\n",
        "\n",
        "#Dataset and loader for testing:\n",
        "test_dataset = CopyTaskDataset(seq_length, vocab_size, num_samples)\n",
        "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
        "\n",
        "#Test the model:\n",
        "test_accuracy = evaluate_LSTM(model, test_loader, chunk_size = 100)\n",
        "print(f\"GRU Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "852d19c6-7cf7-4663-9521-276b1c868f21",
        "id": "wwBEaTYcRh4T"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Loss: 2.256180\n",
            "Epoch 2/10 | Loss: 1.792469\n",
            "Epoch 3/10 | Loss: 1.428502\n",
            "Epoch 4/10 | Loss: 1.323199\n",
            "Epoch 5/10 | Loss: 1.312905\n",
            "Epoch 6/10 | Loss: 1.300798\n",
            "Epoch 7/10 | Loss: 1.293492\n",
            "Epoch 8/10 | Loss: 1.288133\n",
            "Epoch 9/10 | Loss: 1.283637\n",
            "Epoch 10/10 | Loss: 1.279767\n",
            "Training completed in 67.30 seconds\n",
            "Mean Loss: 1.2779\n",
            "GRU Test Accuracy: 0.5476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**T= 100**\n",
        "\n",
        "Epoch 1/10 | Loss: 2.262720<br>\n",
        "Epoch 2/10 | Loss: 1.832611<br>\n",
        "Epoch 3/10 | Loss: 1.478234<br>\n",
        "Epoch 4/10 | Loss: 1.320243<br>\n",
        "Epoch 5/10 | Loss: 1.310087<br>\n",
        "Epoch 6/10 | Loss: 1.299058<br>\n",
        "Epoch 7/10 | Loss: 1.291252<br>\n",
        "Epoch 8/10 | Loss: 1.285523<br>\n",
        "Epoch 9/10 | Loss: 1.280829<br>\n",
        "Epoch 10/10 | Loss: 1.276729<br>\n",
        "Training completed in 65.63 seconds<br>\n",
        "Mean Loss: 1.2747<br>\n",
        "GRU Test Accuracy: 0.5478<br>\n",
        "\n",
        "**T = 200**\n",
        "\n",
        "Epoch 1/10 | Loss: 2.277316<br>\n",
        "Epoch 2/10 | Loss: 1.848805<br>\n",
        "Epoch 3/10 | Loss: 1.488620<br>\n",
        "Epoch 4/10 | Loss: 1.327172<br>\n",
        "Epoch 5/10 | Loss: 1.310578<br>\n",
        "Epoch 6/10 | Loss: 1.297373<br>\n",
        "Epoch 7/10 | Loss: 1.289892<br>\n",
        "Epoch 8/10 | Loss: 1.284580<br>\n",
        "Epoch 9/10 | Loss: 1.279979<br>\n",
        "Epoch 10/10 | Loss: 1.275893<br>\n",
        "Training completed in 67.72 seconds<br>\n",
        "Mean Loss: 1.2736<br>\n",
        "GRU Test Accuracy: 0.5476<br>\n",
        "\n",
        "**T=500**\n",
        "\n",
        "Epoch 1/10 | Loss: 2.193659<br>\n",
        "Epoch 2/10 | Loss: 1.634995<br>\n",
        "Epoch 3/10 | Loss: 1.349776<br>\n",
        "Epoch 4/10 | Loss: 1.319150<br>\n",
        "Epoch 5/10 | Loss: 1.306936<br>\n",
        "Epoch 6/10 | Loss: 1.296261<br>\n",
        "Epoch 7/10 | Loss: 1.288920<br>\n",
        "Epoch 8/10 | Loss: 1.283386<br>\n",
        "Epoch 9/10 | Loss: 1.278781<br>\n",
        "Epoch 10/10 | Loss: 1.274840<br>\n",
        "Training completed in 66.03 seconds<br>\n",
        "Mean Loss: 1.2733<br>\n",
        "GRU Test Accuracy: 0.5478<br>\n",
        "\n",
        "**T = 1000**\n",
        "\n",
        "Epoch 1/10 | Loss: 2.256180<br>\n",
        "Epoch 2/10 | Loss: 1.792469<br>\n",
        "Epoch 3/10 | Loss: 1.428502<br>\n",
        "Epoch 4/10 | Loss: 1.323199<br>\n",
        "Epoch 5/10 | Loss: 1.312905<br>\n",
        "Epoch 6/10 | Loss: 1.300798<br>\n",
        "Epoch 7/10 | Loss: 1.293492<br>\n",
        "Epoch 8/10 | Loss: 1.288133<br>\n",
        "Epoch 9/10 | Loss: 1.283637<br>\n",
        "Epoch 10/10 | Loss: 1.279767<br>\n",
        "Training completed in 67.30 seconds<br>\n",
        "Mean Loss: 1.2779<br>\n",
        "GRU Test Accuracy: 0.5476<br>"
      ],
      "metadata": {
        "id": "un07WN6AGTfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multiplicative GRU\n",
        "\n",
        "Lastly, we perform the training and tests on the multiplicative variant of the GRU model."
      ],
      "metadata": {
        "id": "cRAsbu4ys-yF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader  #Dataset and Dataloader used for training data\n",
        "import time                   #For training time calculation\n",
        "import numpy as np\n",
        "\n",
        "#This function generates a dataset of integer symbols, to be used for training the models\n",
        "class CopyTaskDataset(Dataset):\n",
        "    def __init__(self, seq_length=100, vocab_size=10, num_samples=1000):\n",
        "        self.seq_length = seq_length            #1000 pairs of sequences of len 100 and 10 type of tokens\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_samples = num_samples\n",
        "        self.delimiter_token = vocab_size\n",
        "        self.input_size = vocab_size + 1             #Number of unique tokens + Delimiter\n",
        "        self.data, self.labels = self.generate_data()\n",
        "\n",
        "    def generate_data(self):\n",
        "        data = []\n",
        "        labels = []\n",
        "        for _ in range(self.num_samples):\n",
        "            sequence = np.random.randint(0, self.vocab_size, size=(self.seq_length))\n",
        "\n",
        "            input_seq = np.concatenate([sequence, [self.delimiter_token] * self.seq_length])  #Original sequence first and delimiter tokens follow\n",
        "\n",
        "            target_seq = np.concatenate([[self.delimiter_token] * self.seq_length, sequence])   #Delimiter tokens first and original sequence follows\n",
        "\n",
        "            data.append(input_seq)\n",
        "            labels.append(target_seq)\n",
        "\n",
        "        # We reshape the data to have 3 dimensions and convert them to pytorch tensors (num_samples, seq_length, 1)\n",
        "        data = torch.tensor(data, dtype=torch.long)\n",
        "        labels = torch.tensor(labels, dtype=torch.long)  #long indices for CrossEntropy calculation\n",
        "\n",
        "        data_one_hot = torch.nn.functional.one_hot(data, num_classes=self.input_size).float()      #One hot encoding for training\n",
        "        #labels_one_hot = torch.nn.functional.one_hot(labels, num_classes=self.input_size).float()\n",
        "        return data_one_hot, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "###\n",
        "###mGRU Implementation\n",
        "class LSTMCellPytorch(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A single-step RNN cell in PyTorch for reference:\n",
        "    h_t = tanh( x_t * W_x + h_{t-1} * W_h + b )\n",
        "\n",
        "    Multiplicative GRU:\n",
        "    m_t = sigmoid(x_t * W_xm + h_{t-1} * W_hm + b_m)\n",
        "    x_t = x_t * m_t\n",
        "\n",
        "    z_t = sigmoid(x_t * W_xz + h_{t-1} * W_hz + b_z)\n",
        "    r_t = sigmoid(x_t * W_xr + h_{t-1} * W_hr + b_r)\n",
        "    ~h_t = tanh(z_t * r_t * h_{t-1} + x_t * W_xh + b_h)\n",
        "    h_t = (1 - z_t) * h_{t-1} + z_t * ~h_t\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        #Update gate:\n",
        "        self.W_xz = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.U_hz = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_z = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Reset gate:\n",
        "        self.W_xr = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.U_hr = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_r = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Candidate hidden state:\n",
        "        self.W_xh = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.U_hh = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_h = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_m = torch.nn.Parameter(torch.randn(input_size, hidden_size) * 0.1)\n",
        "        self.U_m = torch.nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)\n",
        "        self.b_m = torch.nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, x_t, h_prev):\n",
        "\n",
        "        z_t = torch.sigmoid(x_t @ self.W_xz + h_prev @ self.U_hz + self.b_z)\n",
        "\n",
        "\n",
        "        r_t = torch.sigmoid(x_t @ self.W_xr + h_prev @ self.U_hr + self.b_r)\n",
        "\n",
        "        m_t = torch.sigmoid(x_t @ self.W_m + h_prev @ self.U_m + self.b_m)\n",
        "\n",
        "        h_candidate = torch.tanh(x_t @ self.W_xh + (r_t * h_prev) @ self.U_hh + self.b_h)\n",
        "\n",
        "\n",
        "        h_t = (1 - z_t) * h_prev + z_t * h_candidate\n",
        "\n",
        "        return z_t, r_t, h_candidate, h_t\n",
        "\n",
        "###\n",
        "#GRU Unrolling over time function\n",
        "class LSTMPyTorch(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Unrolls the GRU over a full sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn_cell = LSTMCellPytorch(input_size, hidden_size)\n",
        "        # Output projection to match the original input dimension for copy task\n",
        "        self.W_out = torch.nn.Parameter(torch.randn(hidden_size, input_size) * 0.1)\n",
        "\n",
        "        self.b_out = torch.nn.Parameter(torch.zeros(input_size))\n",
        "\n",
        "    def forward(self, X, hidden_state= None):\n",
        "        # X: [batch_size, seq_length, input_size]\n",
        "        batch_size, seq_length, _ = X.shape\n",
        "\n",
        "        if hidden_state is None:\n",
        "            h = torch.zeros(batch_size, self.hidden_size, device=X.device)\n",
        "\n",
        "        else:\n",
        "            h = hidden_state\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_length):\n",
        "            x_t = X[:, t, :]  # [batch_size, input_size]\n",
        "            z_t, r_t, h_candidate, h_t = self.rnn_cell(x_t, h)\n",
        "            # Project hidden -> input_size\n",
        "            out_t = h @ self.W_out + self.b_out\n",
        "            outputs.append(out_t.unsqueeze(1))  # shape [batch_size,1,input_size]\n",
        "\n",
        "            h = h_t\n",
        "        # Concatenate across time\n",
        "        return torch.cat(outputs, dim=1), h  # [batch_size, seq_length, input_size]\n",
        "\n",
        "###\n",
        "\n",
        "# The training function- train_LSTM\n",
        "def train_LSTM(model, train_loader, epochs=10, lr=0.01):\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    hidden_state = None        #hidden state maintained during training\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_batches = 0\n",
        "\n",
        "        for X_batch, Y_batch in train_loader:\n",
        "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output, hidden_state = model(X_batch, hidden_state)\n",
        "            hidden_state = (hidden_state.detach())   #Detaches from previous hidden state to prevent exploding gradient\n",
        "\n",
        "            loss = criterion(output.view(-1, input_size), Y_batch.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_batches += 1\n",
        "\n",
        "        epoch_loss = total_loss / total_batches\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "    return time.time() - start_time, loss\n",
        "\n",
        "#The testing function- evaluate_LSTM:\n",
        "def evaluate_LSTM(model, test_loader, chunk_size = 100):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, Y_batch in test_loader:\n",
        "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "            batch_size, seq_length, _ = X_batch.shape\n",
        "            hidden_state = None      #hiddnt state to be maintained throughout the batches\n",
        "            num_chunks = seq_length // chunk_size\n",
        "\n",
        "            chunk_correct = []\n",
        "            chunk_total = []\n",
        "            outputs = []\n",
        "            for i in range(num_chunks):\n",
        "                start = i * chunk_size\n",
        "                end = (i + 1) * chunk_size\n",
        "\n",
        "                X_chunk = X_batch[:, start:end, :]\n",
        "                Y_chunk = Y_batch[:, start:end]\n",
        "\n",
        "                output, hidden_state = model(X_chunk, hidden_state)\n",
        "                hidden_state = (hidden_state.detach())\n",
        "                outputs.append(output)\n",
        "\n",
        "\n",
        "            outputs = torch.cat(outputs, dim=1)  # Reconstruct full sequence\n",
        "            predicted_labels = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            correct_predictions += (predicted_labels == Y_batch).sum().item()\n",
        "            total_predictions += predicted_labels.numel()\n",
        "\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "###\n",
        "#For training dataset:\n",
        "seq_length = 100\n",
        "vocab_size = 10\n",
        "num_samples = 1500\n",
        "batch_size = 64\n",
        "\n",
        "# Creating dataset and Dataloader for training:\n",
        "dataset = CopyTaskDataset(seq_length, vocab_size, num_samples)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
        "\n",
        "# Define model\n",
        "input_size = vocab_size + 1  # Including delimiter token\n",
        "hidden_size = 128\n",
        "model = LSTMPyTorch(input_size, hidden_size)\n",
        "\n",
        "# Train model:\n",
        "training_time, loss = train_LSTM(model, train_loader, epochs=10, lr=0.005)\n",
        "print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "print(f\"Mean Loss: {loss:.4f}\")\n",
        "###\n",
        "\n",
        "###\n",
        "#For testing dataset:\n",
        "seq_length = 1000\n",
        "vocab_size = 10\n",
        "num_samples = 1500\n",
        "batch_size = 64\n",
        "\n",
        "#Dataset and loader for testing:\n",
        "test_dataset = CopyTaskDataset(seq_length, vocab_size, num_samples)\n",
        "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
        "\n",
        "#Test the model:\n",
        "test_accuracy = evaluate_LSTM(model, test_loader, chunk_size = 100)\n",
        "print(f\"mGRU Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlrrXKxds4e6",
        "outputId": "57bc0860-d47e-49ff-b18d-647a8bb80c7d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Loss: 2.279870\n",
            "Epoch 2/10 | Loss: 1.795358\n",
            "Epoch 3/10 | Loss: 1.421967\n",
            "Epoch 4/10 | Loss: 1.324826\n",
            "Epoch 5/10 | Loss: 1.313365\n",
            "Epoch 6/10 | Loss: 1.301349\n",
            "Epoch 7/10 | Loss: 1.293327\n",
            "Epoch 8/10 | Loss: 1.287402\n",
            "Epoch 9/10 | Loss: 1.282551\n",
            "Epoch 10/10 | Loss: 1.278433\n",
            "Training completed in 70.42 seconds\n",
            "Mean Loss: 1.2768\n",
            "mGRU Test Accuracy: 0.5474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**T = 100**\n",
        "\n",
        "Epoch 1/10 | Loss: 2.255296<br>\n",
        "Epoch 2/10 | Loss: 1.793793<br>\n",
        "Epoch 3/10 | Loss: 1.432387<br>\n",
        "Epoch 4/10 | Loss: 1.313185<br>\n",
        "Epoch 5/10 | Loss: 1.307201<br>\n",
        "Epoch 6/10 | Loss: 1.296796<br>\n",
        "Epoch 7/10 | Loss: 1.289199<br>\n",
        "Epoch 8/10 | Loss: 1.283684<br>\n",
        "Epoch 9/10 | Loss: 1.279179<br>\n",
        "Epoch 10/10 | Loss: 1.275271<br>\n",
        "Training completed in 66.17 seconds<br>\n",
        "Mean Loss: 1.2736<br>\n",
        "mGRU Test Accuracy: 0.5482<br>\n",
        "\n",
        "**T = 200**\n",
        "\n",
        "Epoch 1/10 | Loss: 2.242930<br>\n",
        "Epoch 2/10 | Loss: 1.718653<br>\n",
        "Epoch 3/10 | Loss: 1.367778<br>\n",
        "Epoch 4/10 | Loss: 1.328952<br>\n",
        "Epoch 5/10 | Loss: 1.310786<br>\n",
        "Epoch 6/10 | Loss: 1.297574<br>\n",
        "Epoch 7/10 | Loss: 1.289462<br>\n",
        "Epoch 8/10 | Loss: 1.283538<br>\n",
        "Epoch 9/10 | Loss: 1.278701<br>\n",
        "Epoch 10/10 | Loss: 1.274586<br>\n",
        "Training completed in 72.74 seconds<br>\n",
        "Mean Loss: 1.2726<br>\n",
        "mGRU Test Accuracy: 0.5476<br>\n",
        "\n",
        "**T = 500**\n",
        "\n",
        "Epoch 1/10 | Loss: 2.161476<br>\n",
        "Epoch 2/10 | Loss: 1.637637<br>\n",
        "Epoch 3/10 | Loss: 1.344323<br>\n",
        "Epoch 4/10 | Loss: 1.309537<br>\n",
        "Epoch 5/10 | Loss: 1.301227<br>\n",
        "Epoch 6/10 | Loss: 1.291401<br>\n",
        "Epoch 7/10 | Loss: 1.284444<br>\n",
        "Epoch 8/10 | Loss: 1.279242<br>\n",
        "Epoch 9/10 | Loss: 1.274934<br>\n",
        "Epoch 10/10 | Loss: 1.271261<br>\n",
        "Training completed in 62.78 seconds<br>\n",
        "Mean Loss: 1.2699<br>\n",
        "mGRU Test Accuracy: 0.5479<br>\n",
        "\n",
        "**T = 1000**\n",
        "\n",
        "Epoch 1/10 | Loss: 2.279870<br>\n",
        "Epoch 2/10 | Loss: 1.795358<br>\n",
        "Epoch 3/10 | Loss: 1.421967<br>\n",
        "Epoch 4/10 | Loss: 1.324826<br>\n",
        "Epoch 5/10 | Loss: 1.313365<br>\n",
        "Epoch 6/10 | Loss: 1.301349<br>\n",
        "Epoch 7/10 | Loss: 1.293327<br>\n",
        "Epoch 8/10 | Loss: 1.287402<br>\n",
        "Epoch 9/10 | Loss: 1.282551<br>\n",
        "Epoch 10/10 | Loss: 1.278433<br>\n",
        "Training completed in 70.42 seconds<br>\n",
        "Mean Loss: 1.2768<br>\n",
        "mGRU Test Accuracy: 0.5474<br>"
      ],
      "metadata": {
        "id": "yhebeWrsCNke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Results and Inference:\n",
        "As seen from the training and tests, all of the models perform nearly at the same accuracies, though there were differences in the losses calculated.\n",
        "\n",
        "For the LSTM cell, we see:\n",
        "Although the accuracies remain same for all T values, the losses gradually increase with increasing number of sequence.\n",
        "\n",
        "For the multiplicative LSTM:\n",
        "The losses are seen to be fluctuating with increasing value of T.\n",
        "\n",
        "For GRU:\n",
        "The losses also seem to be fluctuating around the same value for increasing values of T.\n",
        "\n",
        "For multiplicative GRU:\n",
        "The loss values also fluctuate. Accuracies remain near the same value.\n",
        "\n",
        "#Conclusion:\n",
        "From the experiment, we see that the models output similar accuracy and loss values for different lengths. The reason for this may be human error, the sequence lengths not being varied enough to make a difference etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "JIaZjjIWI2pO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References:\n",
        "1. Zhang, J., Lei, Q., & Dhillon, I. S. (2018, March 25). Stabilizing gradients for deep neural networks via efficient SVD parameterization. arXiv.org. https://arxiv.org/abs/1803.09327\n",
        "\n",
        "2. Wikipedia contributors. (2025, January 30). Vanishing gradient problem. Wikipedia. https://en.wikipedia.org/wiki/Vanishing_gradient_problem\n",
        "\n",
        "3. Ngutten. (n.d.). LSTMCopy/LSTM.ipynb at master  ngutten/LSTMCopy. GitHub. https://github.com/ngutten/LSTMCopy/blob/master/LSTM.ipynb\n",
        "\n",
        "4. Omarzai, F. (2024, November 19). LSTM and GRU in depth - Fraidoon Omarzai - medium. Medium. https://medium.com/@fraidoonomarzai99/lstm-and-gru-in-depth-40aba24bfe53\n",
        "\n",
        "5. OpenAI. (2025). ChatGPT [Large language model]. https://chat.openai.com/chat\n",
        "\n",
        "6. Google. (2025). Gemini [Large language model]. https://gemini.google.com/app\n",
        "\n",
        "\n",
        "\n",
        "Note: AI tools were used for debugging and learning purposes only.\n",
        "\n"
      ],
      "metadata": {
        "id": "JFBG5sFRz2Ym"
      }
    }
  ]
}